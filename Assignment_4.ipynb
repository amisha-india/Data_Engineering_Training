{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHNxpkvqGHyx"
      },
      "outputs": [],
      "source": [
        "#Task 1 Data Ingestion\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "spark = SparkSession.builder.appName(\"Product Inventory Ingestion\").getOrCreate()\n",
        "\n",
        "file_path = \"/content/sample_data/tables/product_inventory.csv\"\n",
        "logging.basicConfig(filename='/content/sample_data/logs/inventory_ingestion.log', level=logging.INFO)\n",
        "\n",
        "try:\n",
        "    if os.path.exists(file_path):\n",
        "        product_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
        "        product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/product_inventory_raw\")\n",
        "        logging.info(\"Product inventory ingestion completed successfully.\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"File {file_path} not found.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    logging.error(f\"FileNotFoundError: {str(e)}\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"An error occurred: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 Data Cleaning\n",
        "\n",
        "product_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/product_inventory_raw\")\n",
        "cleaned_df = product_df.na.fill({\"StockQuantity\": 0, \"Price\": 0.0})\n",
        "cleaned_df = cleaned_df.filter(col(\"StockQuantity\") >= 0)\n",
        "\n",
        "cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/product_inventory_cleaned\")\n",
        "print(\"Product inventory cleaning completed successfully.\")\n"
      ],
      "metadata": {
        "id": "L5EVAZJaGUEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 Inventory Analysis\n",
        "from pyspark.sql.functions import col, expr\n",
        "cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/product_inventory_cleaned\")\n",
        "\n",
        "stock_value_df = cleaned_df.withColumn(\"TotalStockValue\", col(\"StockQuantity\") * col(\"Price\"))\n",
        "restock_df = cleaned_df.filter(col(\"StockQuantity\") < 100)\n",
        "\n",
        "stock_value_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/product_inventory_analysis\")\n",
        "restock_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/product_inventory_restock\")\n",
        "print(\"Product inventory analysis completed successfully.\")\n"
      ],
      "metadata": {
        "id": "FVQq2hjzGhkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4 Build an Inventory Pipeline\n",
        "import subprocess\n",
        "import logging\n",
        "logging.basicConfig(filename='/content/sample_data/logs/inventory_pipeline_log.log', level=logging.INFO)\n",
        "\n",
        "notebooks = [\n",
        "    \"/content/sample_data/delta/product_inventory_raw\",\n",
        "    \"/content/sample_data/delta/product_inventory_cleaned\",\n",
        "    \"/content/sample_data/delta/product_inventory_analysis\"\n",
        "]\n",
        "\n",
        "for notebook in notebooks:\n",
        "    try:\n",
        "        subprocess.run([\"databricks\", \"workspace\", \"import\", notebook], check=True)\n",
        "        logging.info(f\"Successfully executed {notebook}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logging.error(f\"Error occurred while executing {notebook}: {e}\")\n"
      ],
      "metadata": {
        "id": "zNEx9dQqGpCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 5\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Inventory Monitoring\").getOrCreate()\n",
        "\n",
        "inventory_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/product_inventory_cleaned\")\n",
        "\n",
        "\n",
        "urgent_restock_df = inventory_df.filter(col(\"StockQuantity\") < 50)\n",
        "\n",
        "if urgent_restock_df.count() > 0:\n",
        "    print(\"Alert: Some products need urgent restocking!\")\n",
        "    urgent_restock_df.show()\n",
        "else:\n",
        "    print(\"No products need urgent restocking at the moment.\")\n"
      ],
      "metadata": {
        "id": "ER4iVcwKG056"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}