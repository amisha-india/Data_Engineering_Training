{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GbExesDCP_v"
      },
      "outputs": [],
      "source": [
        "#Task 1 - Raw Data Ingestion\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import os\n",
        "#weather_data.csv\n",
        "spark = SparkSession.builder.appName(\"Weather Data Ingestion\").getOrCreate()\n",
        "schema = StructType([\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"Date\", DateType(), True),\n",
        "    StructField(\"Temperature\", FloatType(), True),\n",
        "    StructField(\"Humidity\", FloatType(), True)\n",
        "])\n",
        "\n",
        "file_path = \"/content/sample_data/weather_data.csv\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "\n",
        "    weather_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
        "\n",
        "    weather_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/weather_raw\")\n",
        "    print(\"Data ingestion completed successfully.\")\n",
        "else:\n",
        "    print(f\"File {file_path} does not exist.\")\n",
        "    spark.createDataFrame([(\"File not found\",)], [\"Error\"]).write.mode(\"append\").save(\"/content/sample_data/delta/ingestion_logs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 Data Cleaning\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "weather_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/weather_raw\")\n",
        "\n",
        "cleaned_df = weather_df.withColumn(\n",
        "    \"Temperature\", when(col(\"Temperature\").isNull() | (col(\"Temperature\") < -50) | (col(\"Temperature\") > 50), None).otherwise(col(\"Temperature\"))\n",
        ").withColumn(\n",
        "    \"Humidity\", when(col(\"Humidity\").isNull() | (col(\"Humidity\") < 0) | (col(\"Humidity\") > 100), None).otherwise(col(\"Humidity\"))\n",
        ")\n",
        "\n",
        "cleaned_df = cleaned_df.dropna()\n",
        "\n",
        "cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/weather_cleaned\")\n",
        "print(\"Data cleaning completed successfully.\")\n"
      ],
      "metadata": {
        "id": "F3MKVReGCXSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 Data Transition\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/weather_cleaned\")\n",
        "transformed_df = cleaned_df.groupBy(\"City\").agg(\n",
        "    avg(\"Temperature\").alias(\"Average_Temperature\"),\n",
        "    avg(\"Humidity\").alias(\"Average_Humidity\")\n",
        ")\n",
        "\n",
        "transformed_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/weather_transformed\")\n",
        "print(\"Data transformation completed successfully.\")\n"
      ],
      "metadata": {
        "id": "aJ41wbHnCqGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4 Build and Run a Pipeline\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "\n",
        "logging.basicConfig(filename='/content/sample_data/logs/pipeline_log.log', level=logging.INFO)\n",
        "notebooks = [\n",
        "    \"/content/sample_data/delta/weather_raw\",\n",
        "    \"/content/sample_data/delta/weather_cleaned\",\n",
        "    \"/content/sample_data/delta/weather_transformed\"\n",
        "]\n",
        "\n",
        "for notebook in notebooks:\n",
        "    try:\n",
        "        subprocess.run([\"databricks\", \"workspace\", \"import\", notebook], check=True)\n",
        "        logging.info(f\"Successfully executed {notebook}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logging.error(f\"Error occurred while executing {notebook}: {e}\")\n"
      ],
      "metadata": {
        "id": "0hnQN-CtDK9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_uhbxuGDR08"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}