{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e19a58a-2d44-4449-994b-5cf4217115e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  TASK -1 Create Delta Tables Using 3 Methods\n",
    "#1.1. Load the sales_data.csv file into a DataFrame.\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/sales_data.csv\", \"dbfs:/FileStore/sales_data.csv\")\n",
    "df_sales=spark.read.format(\"csv\").option (\"header\", \"true\").load(\"/FileStore/sales_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c34fbc0c-f2db-4f77-9622-de891b06ebe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.2. Write the DataFrame as a Delta Table.\n",
    "df_sales.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/sales_data\")\n",
    "df_sales=spark.read.format(\"delta\").load(\"/delta/sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9727cde-196d-4244-9de9-152c6cd03443",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------+----------+\n|CustomerID| CustomerName|Region|SignupDate|\n+----------+-------------+------+----------+\n|      C001|     John Doe| North|2022-07-01|\n|      C002|   Jane Smith| South|2023-02-15|\n|      C003|Emily Johnson|  East|2021-11-20|\n|      C004|Michael Brown|  West|2022-12-05|\n|      C005|  Linda Davis| North|2023-03-10|\n+----------+-------------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#1.3. Load the customer_data.json file into a DataFrame.\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "schema=StructType([  \n",
    "                   StructField(\"CustomerID\", StringType(), True),\n",
    "                   StructField(\"CustomerName\", StringType(), True),\n",
    "                   StructField(\"Region\", StringType(), True),\n",
    "                   StructField(\"SignupDate\", StringType(), True) \n",
    "])\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\", \"dbfs:/FileStore/customer_data.json\")\n",
    "\n",
    "df_customer=spark.read.format(\"json\").schema(schema).load(\"dbfs:/FileStore/customer_data.json\") \n",
    "df_customer.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f456e13a-2d4b-46aa-9e33-9c9f1e6129f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.4. Write the DataFrame as a Delta Table.\n",
    "df_customer.createOrReplaceTempView(\"customer_view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta_customer\n",
    "USING DELTA\n",
    "AS SELECT * FROM customer_view\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8ccb7a7-2f77-43f8-9c56-2c8c17583443",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.5 Load the Parquet file\n",
    "parquet_df = spark.read.parquet(\"/FileStore/sample_parquet_file.parquet\")\n",
    "\n",
    "# Convert the Parquet file to a Delta Table\n",
    "parquet_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/sample_parquet_to_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da17de50-019a-48ff-94e1-5a0bec6006cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n+-------+----------+----------+--------+--------+-----+\n|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n+-------+----------+----------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# TASK-2 Data Management\n",
    "#2.1. Load the new_sales_data.csv file into a DataFrame.\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/new_sales_data.csv\", \"dbfs:/FileStore/new_sales_data.csv\")\n",
    "df_new_sales=spark.read.format(\"csv\").option (\"header\", \"true\").load(\"/FileStore/new_sales_data.csv\")\n",
    "df_new_sales.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5d8bea-de00-458d-b0d9-063194c31b37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.2. Write the new DataFrame as a Delta Table.\n",
    "df_new_sales.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/new_sales_data\")\n",
    "df_new_sales=spark.read.format(\"delta\").load(\"/delta/new_sales_data\")\n",
    "df_sales.createOrReplaceTempView(\"delta_sales\") \n",
    "df_new_sales.createOrReplaceTempView(\"new_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f4fd70-1f8f-4858-819b-7824effbb6f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n+-------+----------+----------+--------+--------+-----+\n|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n+-------+----------+----------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#2.3. Perform a MERGE INTO operation to update and insert records into the existing Delta table.\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_sales AS target \n",
    "          USING new_sales AS source \n",
    "          ON target.OrderID = source.OrderID \n",
    "          WHEN MATCHED THEN UPDATE SET target.OrderDate = source.OrderDate,target.CustomerID = source.CustomerID,target.Product = source.Product, target.Quantity = source.Quantity, target.Price = source.Price\n",
    "          WHEN NOT MATCHED THEN INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price)\n",
    "          VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
    "\"\"\")\n",
    "spark.sql(\"SELECT * FROM delta_sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39334a10-a52d-4118-9c0c-2a5b2273bb41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task - 3 Optimize Delta Table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_sales_table USING DELTA LOCATION '/delta/sales_data'\")\n",
    "#3.1. Apply the OPTIMIZE command on the Delta Table and use Z-Ordering on an appropriate column.\n",
    "spark.sql(\"\"\"\n",
    "\tOPTIMIZE delta_customer ZORDER BY CustomerID\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\tOPTIMIZE delta_sales_table ZORDER BY Product\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7322f01f-139f-4fb3-8609-3e37d8287453",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation             |operationParameters                                                                                                                                     |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+------------------------------------------+\n|0      |2024-09-13 09:20:41|7325618528585827|azuser2111_mml.local@techademy.com|CREATE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{1302574462417202}|0913-085422-z61vpium|NULL       |WriteSerializable|true         |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1253}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+------------------------------------------+\n\n+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                          |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|5      |2024-09-13 09:27:32|7325618528585827|azuser2111_mml.local@techademy.com|OPTIMIZE |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                               |NULL|{1302574462417202}|0913-085422-z61vpium|4          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 3216, p25FileSize -> 1794, numDeletionVectorsRemoved -> 1, minFileSize -> 1794, numAddedFiles -> 1, maxFileSize -> 1794, p75FileSize -> 1794, p50FileSize -> 1794, numAddedBytes -> 1794}                                                                                                                                                                                                                                                                                                                                                                                                                                                  |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|4      |2024-09-13 09:27:28|7325618528585827|azuser2111_mml.local@techademy.com|MERGE    |{predicate -> [\"(OrderID#2194 = OrderID#3463)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{1302574462417202}|0913-085422-z61vpium|3          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1495, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 4895, materializeSourceTimeMs -> 298, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2795, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1723}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|3      |2024-09-13 09:20:05|7325618528585827|azuser2111_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                 |NULL|{1302574462417202}|0913-085422-z61vpium|2          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2024-09-13 09:19:25|7325618528585827|azuser2111_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                 |NULL|{1302574462417202}|0913-085422-z61vpium|1          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2024-09-13 09:19:06|7325618528585827|azuser2111_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                 |NULL|{1302574462417202}|0913-085422-z61vpium|0          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2024-09-13 09:04:55|7325618528585827|azuser2111_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                 |NULL|{1302574462417202}|0913-085422-z61vpium|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 8, numOutputBytes -> 1721}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# TASK - 4 Advanced Features\n",
    "#4.1. Use DESCRIBE HISTORY to inspect the history of changes for a Delta Table.\n",
    "spark.sql(\"DESCRIBE HISTORY delta_customer\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"DESCRIBE HISTORY delta_sales_table\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d849fb89-5f6b-4d79-ad3e-06300d39c18c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.2. Use VACUUM to remove old files from the Delta Table.\n",
    "spark.sql(\"\"\"\n",
    "\tVACUUM delta_customer RETAIN 168 HOURS\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\tVACUUM delta_sales_table RETAIN 168 HOURS\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290123c8-0c78-4457-ac48-25283e0030b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n+-------+----------+----------+--------+--------+-----+\n|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n|   1002|2024-01-16|      C002|Widget B|       5|15.75|\n|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n+-------+----------+----------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Task-5  Hands-on Exercises\n",
    "#5.1. Using Delta Lake for Data Versioning:\n",
    "df_version = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"delta_sales_table\")\n",
    "df_version.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f14f0cd-bdee-4eb3-b606-4446286ef7ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n+-------+----------+----------+--------+--------+-----+\n|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n+-------+----------+----------+--------+--------+-----+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.2. Building a Reliable Data Lake with Delta Lake:\n",
    "#Implement schema enforcement and handle data updates with Delta Lake.\n",
    "#schema enforcement\n",
    "from pyspark.sql.types import StructType, StructField,IntegerType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), nullable=False),\n",
    "    StructField(\"OrderDate\", StringType(), nullable=False),\n",
    "    StructField(\"CustomerID\", StringType(), nullable=False),\n",
    "    StructField(\"Product\", StringType(), nullable=False),\n",
    "    StructField(\"Quantity\", IntegerType(), nullable=False),\n",
    "    StructField(\"Price\", DoubleType(), nullable=False)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"dbfs:/FileStore/sales_data.csv\", schema=schema, header=True)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/delta_sales_table\")\n",
    "\n",
    "#handle data updates\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_sales AS target\n",
    "          USING new_sales AS source\n",
    "          ON target.OrderID = source.OrderID\n",
    "          WHEN MATCHED THEN UPDATE SET target.OrderDate = source.OrderDate,target.CustomerID = source.CustomerID,target.Product = source.Product, target.Quantity = source.Quantity, target.Price = source.Price\n",
    "          WHEN NOT MATCHED THEN INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price)\n",
    "          VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
    "\"\"\")\n",
    "spark.sql(\"SELECT * FROM delta_sales\").show()\n",
    "\n",
    "\n",
    "#Optimize data layout and perform vacuum operations to maintain storage efficiency\n",
    "spark.sql(\"\"\"\n",
    "\tOPTIMIZE delta_customer ZORDER BY CustomerID\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "\tVACUUM delta_customer RETAIN 168 HOURS\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "build and manage data lake with Databricks Delta Lake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
