{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwiILHHPWrt6",
        "outputId": "737f7b38-7268-4e85-ede9-93274be2f5e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=121c13993b5f9d06419f75204d8e32e1bfb7dd6df3f587e2ad79086fbb197eea\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Creating a Complete ETL Pipeline using Delta Live Tables(DLT)\n",
        "# Step 1: Ingest Raw Data from CSV Files\n",
        "import dlt\n",
        "\n",
        "@dlt.table\n",
        "def raw_transactions():\n",
        "    \"\"\"Ingest raw data from the CSV file.\"\"\"\n",
        "    return spark.read.csv(\"/content/sample_data/transactions.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 2: Apply Transformations\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "@dlt.table\n",
        "def transformed_transactions():\n",
        "    \"\"\"Transform data by calculating the TotalAmount.\"\"\"\n",
        "    return (\n",
        "        dlt.read(\"raw_transactions\")\n",
        "        .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
        "    )\n",
        "# Step 3: Write the Final Data into a Delta Table\n",
        "@dlt.table\n",
        "def final_transactions():\n",
        "    \"\"\"Write the final data into a Delta table.\"\"\"\n",
        "    return dlt.read(\"transformed_transactions\")\n"
      ],
      "metadata": {
        "id": "4ZgGYZoJYGPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write DLT in Python\n",
        "import dlt\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "@dlt.table\n",
        "def raw_transactions():\n",
        "    # Step 1: Read data from the CSV file\n",
        "    return spark.read.csv(\"/content/sample_data/transactions.csv\", header=True, inferSchema=True)\n",
        "\n",
        "@dlt.table\n",
        "def transformed_transactions():\n",
        "    # Step 2: Apply transformations to calculate total transaction amount\n",
        "    return (\n",
        "        dlt.read(\"raw_transactions\")\n",
        "        .select(\n",
        "            col(\"TransactionID\"),\n",
        "            col(\"TransactionDate\"),\n",
        "            col(\"CustomerID\"),\n",
        "            col(\"Product\"),\n",
        "            col(\"Quantity\"),\n",
        "            col(\"Price\"),\n",
        "            (col(\"Quantity\") * col(\"Price\")).alias(\"TotalAmount\")  # Calculate total amount\n",
        "        )\n",
        "    )\n"
      ],
      "metadata": {
        "id": "LbMKg0o3YKY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "-- Step 1: Create Raw Transactions Table\n",
        "CREATE OR REFRESH LIVE TABLE raw_transactions AS\n",
        "SELECT *\n",
        "FROM read_csv('/content/sample_data/transactions.csv', header = true);\n",
        "\n",
        "-- Step 2: Create Transformed Transactions Table\n",
        "CREATE OR REFRESH LIVE TABLE transformed_transactions AS\n",
        "SELECT\n",
        "    TransactionID,\n",
        "    TransactionDate,\n",
        "    CustomerID,\n",
        "    Product,\n",
        "    Quantity,\n",
        "    Price,\n",
        "    Quantity * Price AS TotalAmount  -- Calculate total amount\n",
        "FROM\n",
        "    LIVE.raw_transactions;\n",
        "'''"
      ],
      "metadata": {
        "id": "5KAiDNaTYPQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Monitor the Pipeline\n",
        "# 1. Access the DLT UI:\n",
        "\n",
        "#   Open the Databricks workspace.\n",
        "\n",
        "#   Click on \"Workflows\" in the sidebar.\n",
        "\n",
        "#   Select \"Delta Live Tables.\"\n",
        "\n",
        "# 2. View the Pipeline:\n",
        "\n",
        "#   A list of DLT pipelines will be displayed. Click on the desired pipeline to view its details.\n",
        "\n",
        "#   The status of each table can be checked, along with any errors or warnings.\n",
        "\n",
        "# 3. Examine Execution Details:\n",
        "\n",
        "#   Execution history, logs, and performance metrics for each step in the pipeline can be reviewed."
      ],
      "metadata": {
        "id": "wXd_yie7YZFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YcuKUpW6YbOW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}