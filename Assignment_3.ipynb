{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtFqBKAqDyrq"
      },
      "outputs": [],
      "source": [
        "#Task 1 Data Ingestion\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "import logging\n",
        "spark = SparkSession.builder.appName(\"Customer Data Ingestion\").getOrCreate()\n",
        "file_path = \"/content/sample_data/customer_transactions.csv\"\n",
        "\n",
        "logging.basicConfig(filename='/content/sample_data/logs/ingestion_log.log', level=logging.INFO)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    customer_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
        "\n",
        "    customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/customer_raw\")\n",
        "    logging.info(\"Customer data ingestion completed successfully.\")\n",
        "else:\n",
        "    logging.error(f\"File {file_path} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2 Data Cleaning\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "customer_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/customer_raw\")\n",
        "cleaned_df = customer_df.dropDuplicates()\n",
        "cleaned_df = cleaned_df.na.fill({\"TransactionAmount\": 0})\n",
        "\n",
        "\n",
        "cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/customer_cleaned\")\n",
        "print(\"Customer data cleaning completed successfully.\")\n"
      ],
      "metadata": {
        "id": "lSWN64ihEDLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 Data Aggregation\n",
        "from pyspark.sql.functions import sum\n",
        "cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/customer_cleaned\")\n",
        "\n",
        "aggregated_df = cleaned_df.groupBy(\"ProductCategory\").agg(\n",
        "    sum(\"TransactionAmount\").alias(\"TotalTransactionAmount\")\n",
        ")\n",
        "\n",
        "aggregated_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/customer_aggregated\")\n",
        "print(\"Customer data aggregation completed successfully.\")\n"
      ],
      "metadata": {
        "id": "pGChxlJyEUxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4 Pipeline Creation\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='/content/sample_data/logs/pipeline_log.log', level=logging.INFO)\n",
        "notebooks = [\n",
        "    \"/content/sample_data/delta/customer_raw\",\n",
        "    \"/content/sample_data/delta/customer_cleaned\",\n",
        "    \"/content/sample_data/delta/customer_aggregated\"\n",
        "]\n",
        "for notebook in notebooks:\n",
        "    try:\n",
        "        subprocess.run([\"databricks\", \"workspace\", \"import\", notebook], check=True)\n",
        "        logging.info(f\"Successfully executed {notebook}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logging.error(f\"Error occurred while executing {notebook}: {e}\")\n"
      ],
      "metadata": {
        "id": "2tnateVQEkFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 5\n",
        "\n",
        "cleaned_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/customer_cleaned\")\n",
        "aggregated_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/customer_aggregated\")\n",
        "\n",
        "total_transactions = cleaned_df.agg(sum(\"TransactionAmount\").alias(\"TotalTransactions\")).collect()[0][\"TotalTransactions\"]\n",
        "\n",
        "total_aggregated_transactions = aggregated_df.agg(sum(\"TotalTransactionAmount\").alias(\"TotalAggregatedTransactions\")).collect()[0][\"TotalAggregatedTransactions\"]\n",
        "\n",
        "if total_transactions == total_aggregated_transactions:\n",
        "    print(f\"Data validation passed: {total_transactions} == {total_aggregated_transactions}\")\n",
        "else:\n",
        "    print(f\"Data validation failed: {total_transactions} != {total_aggregated_transactions}\")\n"
      ],
      "metadata": {
        "id": "VgAftEkiEpEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}