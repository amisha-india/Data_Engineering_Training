{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmAmjjH1VvhF"
      },
      "outputs": [],
      "source": [
        "# Task 1: Data Ingestion - Reading Data from Various Formats\n",
        "\n",
        "csv_file_path = \"dbfs:/FileStore/student_info.csv\"\n",
        "\n",
        "student_df = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
        "\n",
        "json_file_path = \"dbfs:/FileStore/city_info.json\"\n",
        "\n",
        "city_df = spark.read.option(\"multiline\", \"true\").json(json_file_path)\n",
        "# File path for Parquet\n",
        "parquet_file_path = \"dbfs:/FileStore/hospital_info.parquet\"\n",
        "\n",
        "# Read Parquet data\n",
        "hospital_parquet_df = spark.read.parquet(parquet_file_path)\n",
        "\n",
        "try:\n",
        "    # Read Delta table\n",
        "    hospital_delta_df = spark.read.format(\"delta\").load(\"/delta/hospital_records\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Delta table: {e}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Writing Data to Various Formats\n",
        "student_df.write.option(\"header\", \"true\").csv(\"dbfs:/FileStore/output_student.csv\")\n",
        "\n",
        "city_df.write.json(\"dbfs:/FileStore/output_city.json\")\n",
        "\n",
        "hospital_parquet_df.write.parquet(\"dbfs:/FileStore/output_hospital.parquet\")\n",
        "\n",
        "hospital_parquet_df.write.format(\"delta\").save(\"/delta/output_hospital\")"
      ],
      "metadata": {
        "id": "S53EdUd0WEjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Running One Notebook from Another\n",
        "# Notebook A (Data Ingestion & Cleaning):\n",
        "# Load CSV\n",
        "student_df = spark.read.option(\"header\", \"true\").csv(\"dbfs:/FileStore/student_info.csv\")\n",
        "\n",
        "# Clean data: remove duplicates\n",
        "student_cleaned_df = student_df.dropDuplicates()\n",
        "\n",
        "# Write cleaned data to Delta table\n",
        "student_cleaned_df.write.format(\"delta\").save(\"/delta/student_cleaned\")\n"
      ],
      "metadata": {
        "id": "uwQGHK9iWHHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook B (Data Analysis):\n",
        "# Load Delta table from Notebook A\n",
        "student_df = spark.read.format(\"delta\").load(\"/delta/student_cleaned\")\n",
        "\n",
        "# Perform analysis\n",
        "avg_score_df = student_df.groupBy(\"Class\").avg(\"Score\").withColumnRenamed(\"avg(Score)\", \"AverageScore\")\n",
        "\n",
        "# Write the analysis to a Delta table\n",
        "avg_score_df.write.format(\"delta\").save(\"/delta/student_analysis\")\n"
      ],
      "metadata": {
        "id": "2jvZpJDsWLph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Databricks Ingestion\n",
        "student_df = spark.read.option(\"header\", \"true\").csv(/student_info.csv)\n",
        "\n",
        "json_file_path = \"/FileStore/city_info.json\"\n",
        "city_df = spark.read.option(\"multiline\", \"true\").json(json_file_path)\n",
        "\n",
        "delta_df = spark.read.format(\"delta\").load(\"/delta/hospital_records\")\n"
      ],
      "metadata": {
        "id": "Z1LhSAo-WOW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Optimization, Z-ordering, and VACUUM\n",
        "spark.sql(\"OPTIMIZE '/delta/output_hospital'\")\n",
        "\n",
        "spark.sql(\"OPTIMIZE '/delta/output_hospital' ZORDER BY (CityName)\")\n",
        "\n",
        "spark.sql(\"VACUUM '/delta/output_hospital' RETAIN 0 HOURS\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CKwwSfeNWTl4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}