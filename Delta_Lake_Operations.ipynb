{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNEeuxByYyJK",
        "outputId": "0fe7d9f4-0925-4321-fcc1-a261c47a2111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=4db2ef1ff8e97f6c182b69c84567e886e67940d41457f2e7e0135bb1434c5d74\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: Delta Lake Operations - Read, Write, Update, Delete, Merge\n",
        "# 1: Read Data from Delta Lake\n",
        "# Read the transactional data from the Delta table\n",
        "transactions_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/final_transactions\")\n",
        "\n",
        "# Display the first 5 rows\n",
        "transactions_df.show(5)"
      ],
      "metadata": {
        "id": "iUEbpKyVY6m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Write Data to Delta Lake\n",
        "# Define new transactions to append\n",
        "new_transactions = [\n",
        "    (6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n",
        "    (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)\n",
        "]\n",
        "\n",
        "# Create a DataFrame for new transactions\n",
        "new_transactions_df = spark.createDataFrame(new_transactions, [\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n",
        "\n",
        "# Append new transactions to the Delta table\n",
        "new_transactions_df.write.format(\"delta\").mode(\"append\").save(\"/content/sample_data/delta/final_transactions\")\n"
      ],
      "metadata": {
        "id": "Agl7cqlfY9am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3: Update Data in Delta Lake\n",
        "from delta.tables import *\n",
        "\n",
        "# Load the Delta table\n",
        "delta_table = DeltaTable.forPath(spark, \"/content/sample_data/delta/final_transactions\")\n",
        "\n",
        "# Update the Price of Product 'Laptop'\n",
        "delta_table.update(\n",
        "    condition=\"Product = 'Laptop'\",\n",
        "    set={\"Price\": \"1300\"}\n",
        ")\n",
        "\n",
        "# Verify the update\n",
        "transactions_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/final_transactions\")\n",
        "transactions_df.filter(\"Product = 'Laptop'\").show()\n"
      ],
      "metadata": {
        "id": "H0PEki6LZBFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Delete Data from Delta Lake\n",
        "# Delete all transactions where Quantity is less than 3\n",
        "delta_table.delete(\"Quantity < 3\")\n",
        "\n",
        "# Verify the deletion\n",
        "transactions_df = spark.read.format(\"delta\").load(\"/content/sample_data/delta/final_transactions\")\n",
        "transactions_df.show()"
      ],
      "metadata": {
        "id": "ncKyTLAiZDHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Merge Data into Delta Lake\n",
        "# Create new data for merging\n",
        "merge_data = [\n",
        "    (1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1250),  # Updated Price\n",
        "    (8, \"2024-09-08\", \"C007\", \"Charger\", 2, 30)    # New Transaction\n",
        "]\n",
        "\n",
        "# Create a DataFrame for the merge data\n",
        "merge_df = spark.createDataFrame(merge_data, [\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n",
        "\n",
        "# Perform the merge operation\n",
        "delta_table.alias(\"t\").merge(\n",
        "    merge_df.alias(\"s\"),\n",
        "    \"t.TransactionID = s.TransactionID\"\n",
        ").whenMatchedUpdate(set={\n",
        "    \"Price\": \"s.Price\",\n",
        "    \"Quantity\": \"s.Quantity\",\n",
        "    \"TransactionDate\": \"s.TransactionDate\",\n",
        "    \"CustomerID\": \"s.CustomerID\",\n",
        "    \"Product\": \"s.Product\"\n",
        "}).whenNotMatchedInsert(values={\n",
        "    \"TransactionID\": \"s.TransactionID\",\n",
        "    \"TransactionDate\": \"s.TransactionDate\",\n",
        "    \"CustomerID\": \"s.CustomerID\",\n",
        "    \"Product\": \"s.Product\",\n",
        "    \"Quantity\": \"s.Quantity\",\n",
        "    \"Price\": \"s.Price\"\n",
        "}).execute()\n"
      ],
      "metadata": {
        "id": "zblkn-wWZHs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}