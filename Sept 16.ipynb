{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c463be6-2135-40d2-a27c-cebf8227fe9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CustomerID: string (nullable = true)\n |-- CustomerName: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- SignupDate: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/Sales_data3.csv\", \"dbfs:/FileStore/streaming/input/sales_data.csv\")\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data1.json\", \"dbfs:/FileStore/streaming/input/customer_data.json\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreamingExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the CSV data\n",
    "sales_schema = \"OrderID INT, OrderDate STRING, CustomerID STRING, Product STRING, Quantity INT, Price DOUBLE\"\n",
    "\n",
    "# Read streaming data from CSV files\n",
    "df_sales_stream = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(sales_schema) \\\n",
    "    .load(\"dbfs:/FileStore/streaming/input/\")\n",
    "\n",
    "# Define the schema for the JSON data\n",
    "customer_schema = \"CustomerID STRING, CustomerName STRING, Region STRING, SignupDate STRING\"\n",
    "\n",
    "# Read streaming data from JSON files\n",
    "df_customers_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(customer_schema) \\\n",
    "    .load(\"dbfs:/FileStore/streaming/input/\")\n",
    "\n",
    "df_customers_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2076426e-a8ff-4b6c-974e-c5492f20ca33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied transformations on sales data...\nAggregated sales data by product...\nApplied transformations on customer data...\nStarted streaming query to write aggregated sales data to console...\nStarted streaming query to write transformed customer data to console...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, datediff, to_timestamp\n",
    "\n",
    "# Transform the sales data: Add a new column for total amount\n",
    "df_sales_transformed = df_sales_stream.select(\n",
    "    col(\"OrderID\"),\n",
    "    to_timestamp(col(\"OrderDate\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"OrderDate\"), # Convert OrderDate to TIMESTAMP\n",
    "    col(\"Product\"),\n",
    "    col(\"Quantity\"),\n",
    "    col(\"Price\"),\n",
    "    (col(\"Quantity\") * col(\"Price\")).alias(\"TotalAmount\")\n",
    ")\n",
    "\n",
    "print(\"Applied transformations on sales data...\")\n",
    "\n",
    "# Add watermark to handle late data and perform an aggregation\n",
    "df_sales_aggregated = df_sales_transformed \\\n",
    "    .withWatermark(\"OrderDate\", \"10 days\") \\\n",
    "    .groupBy(\"Product\") \\\n",
    "    .agg({\"TotalAmount\": \"sum\"})\n",
    "\n",
    "print(\"Aggregated sales data by product...\")\n",
    "\n",
    "# Transform the customer data: Add a new column for the number of years since signup\n",
    "df_customers_transformed = df_customers_stream.withColumn(\n",
    "    \"YearsSinceSignup\",\n",
    "    datediff(current_date(), to_timestamp(col(\"SignupDate\"), \"yyyy-MM-dd\")).cast(\"int\") / 365\n",
    ")\n",
    "\n",
    "print(\"Applied transformations on customer data...\")\n",
    "# Write the aggregated sales data to a console sink for debugging\n",
    "sales_query = df_sales_aggregated.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Started streaming query to write aggregated sales data to console...\")\n",
    "\n",
    "# Write the transformed customer data to a console sink for debugging\n",
    "customers_query = df_customers_transformed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Started streaming query to write transformed customer data to console...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838fb889-fd1c-4caf-bf6c-589284ee8403",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data saved to /dbfs/FileStore/sales_data.csv\nData Loaded Successfully\nData Transformed Successfully\nTransformed data written to Delta table successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Create sample sales data\n",
    "data = {\n",
    "    \"OrderID\": [1, 2, 3, 4],\n",
    "    \"OrderDate\": [\"2024-01-01 10:00:00\", \"2024-01-02 11:00:00\", \"2024-01-03 12:00:00\", \"2024-01-04 13:00:00\"],\n",
    "    \"CustomerID\": [\"C001\", \"C002\", \"C003\", \"C004\"],\n",
    "    \"Product\": [\"ProductA\", \"ProductB\", \"ProductC\", \"ProductD\"],\n",
    "    \"Quantity\": [10, 20, 15, 5],\n",
    "    \"Price\": [100.0, 200.0, 150.0, 50.0]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"/dbfs/FileStore/sales_data.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Sample data saved to {csv_path}\")\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreamingExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/sales_data.csv\")\n",
    "\n",
    "print(\"Data Loaded Successfully\")\n",
    "\n",
    "# Transform the data: Add a new column for total amount\n",
    "df_transformed = df.withColumn(\"TotalAmount\", col(\"Quantity\").cast(\"int\") * col(\"Price\").cast(\"double\"))\n",
    "\n",
    "print(\"Data Transformed Successfully\")\n",
    "\n",
    "# Write transformed data to a Delta table\n",
    "df_transformed.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/sales_data\")\n",
    "\n",
    "print(\"Transformed data written to Delta table successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b34553-a1c7-4681-af9c-20d3305f381a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data saved to /dbfs/FileStore/sales_data.csv and /dbfs/FileStore/sales_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample sales data\n",
    "sales_data = {\n",
    "    \"OrderID\": [1, 2, 3, 4],\n",
    "    \"OrderDate\": [\"2024-01-01 10:00:00\", \"2024-01-02 11:00:00\", \"2024-01-03 12:00:00\", \"2024-01-04 13:00:00\"],\n",
    "    \"CustomerID\": [\"C001\", \"C002\", \"C003\", \"C004\"],\n",
    "    \"Product\": [\"ProductA\", \"ProductB\", \"ProductC\", \"ProductD\"],\n",
    "    \"Quantity\": [10, 20, 15, 5],\n",
    "    \"Price\": [100.0, 200.0, 150.0, 50.0]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = \"/dbfs/FileStore/sales_data.csv\"\n",
    "df_sales.to_csv(csv_path, index=False)\n",
    "\n",
    "# Save as Parquet\n",
    "parquet_path = \"/dbfs/FileStore/sales_data.parquet\"\n",
    "df_sales.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"Sample data saved to {csv_path} and {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2dd1cd1-0632-4e87-8f73-1e3b520563ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table created and data written successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from CSV\n",
    "df_sales = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/sales_data.csv\")\n",
    "\n",
    "# Transform the data: Add a new column for total amount\n",
    "df_transformed = df_sales.withColumn(\"TotalAmount\", col(\"Quantity\").cast(\"int\") * col(\"Price\").cast(\"double\"))\n",
    "\n",
    "# Write transformed data to Delta table\n",
    "delta_table_path = \"/delta/sales_data\"\n",
    "df_transformed.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Delta table created and data written successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f6b1ca-da15-4b37-ad78-7e53fa675248",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta Live Table created.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>sales_data</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>OrderID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>OrderDate</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>CustomerID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Product</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Quantity</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Price</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>TotalAmount</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=3929522205148641#joblist/pipelines/create?initialSource=%2FUsers%2Fazuser2110_mml.local%40techademy.com%2FSept%2016&redirectNotebookId=2517788227427059\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "@dlt.table\n",
    "def sales_data():\n",
    "    df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    return df.select(\n",
    "        col(\"OrderID\"),\n",
    "        col(\"OrderDate\"),\n",
    "        col(\"CustomerID\"),\n",
    "        col(\"Product\"),\n",
    "        col(\"Quantity\"),\n",
    "        col(\"Price\"),\n",
    "        (col(\"Quantity\").cast(\"int\") * col(\"Price\").cast(\"double\")).alias(\"TotalAmount\")\n",
    "    )\n",
    "\n",
    "print(\"Delta Live Table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11ffc33-32b0-4a95-872e-27281ff54fec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Delta table created and data written successfully.\nNew data appended to Delta table successfully.\nMerging new data into Delta table...\nData merged successfully.\nViewing Delta table history...\n+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation|operationParameters                                                                                                                                                                                |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|2      |2024-09-16 12:00:51|8532116595080723|azuser2110_mml.local@techademy.com|MERGE    |{predicate -> [\"(ID#683L = ID#403L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{2517788227427059}|0911-102441-r0fo913u|1          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 1240, numTargetBytesRemoved -> 1772, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 5497, materializeSourceTimeMs -> 433, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 3816, numTargetRowsUpdated -> 3, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 3, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1181}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2024-09-16 12:00:40|8532116595080723|azuser2110_mml.local@techademy.com|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}                                                                                                                                          |NULL|{2517788227427059}|0911-102441-r0fo913u|0          |WriteSerializable|true         |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1181}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2024-09-16 12:00:34|8532116595080723|azuser2110_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                       |NULL|{2517788227427059}|0911-102441-r0fo913u|NULL       |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 1773}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\nQuerying Delta table as of version 0...\n+---+-----+\n|ID |Value|\n+---+-----+\n|1  |100  |\n|2  |200  |\n|3  |300  |\n+---+-----+\n\nVacuuming old files...\nDelta operations completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaOperationsSimpleExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Delta table path\n",
    "delta_table_path = \"/delta/simple_data\"\n",
    "\n",
    "# Define initial sample data\n",
    "initial_data = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 300)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"ID\", \"Value\"]\n",
    "\n",
    "# Create DataFrame for initial data\n",
    "df_initial = spark.createDataFrame(initial_data, schema=schema)\n",
    "\n",
    "# Write DataFrame to Delta table\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Initial Delta table created and data written successfully.\")\n",
    "\n",
    "# Define new sample data\n",
    "new_sample_data = [\n",
    "    (2, 250),  # Existing ID with updated Value\n",
    "    (4, 400)   # New ID\n",
    "]\n",
    "\n",
    "# Create DataFrame for new data\n",
    "df_new = spark.createDataFrame(new_sample_data, schema=schema)\n",
    "\n",
    "# Write the new data to Delta table in append mode\n",
    "df_new.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "print(\"New data appended to Delta table successfully.\")\n",
    "\n",
    "# Create a temporary view for SQL operations\n",
    "df_new.createOrReplaceTempView(\"new_data\")\n",
    "\n",
    "# Perform the merge operation\n",
    "print(\"Merging new data into Delta table...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO delta.`{delta_table_path}` AS target\n",
    "USING new_data AS source\n",
    "ON target.ID = source.ID\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.Value = source.Value\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "    ID,\n",
    "    Value\n",
    ") VALUES (\n",
    "    source.ID,\n",
    "    source.Value\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data merged successfully.\")\n",
    "\n",
    "# Delta operations - History, Time Travel, and Vacuum\n",
    "print(\"Viewing Delta table history...\")\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\")\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "print(\"Querying Delta table as of version 0...\")\n",
    "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "df_time_travel.show(truncate=False)\n",
    "\n",
    "print(\"Vacuuming old files...\")\n",
    "spark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 168 HOURS\")\n",
    "\n",
    "print(\"Delta operations completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sept 16",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
